{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Принцип работы YOLO"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image](Rn-V9RsehT8.jpg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Как происходит детекция\n",
    "---\n",
    "Для каждой клетки определяется класс и BB объекта в центре. На этапе инициализации сеть определяет размеры anchor box которые являются возможными вариантами Bounding box"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image](0f_-1wtt4qkwxr6bww4hpysnuwc.jpeg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Пример anchor box"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image](fxzqqy3rgqs_fh2s7ajninpcxos.jpeg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Выход каждой клетки\n",
    "---\n",
    "Для каждой клетки, нам нужно понять две принципиальные вещи:\n",
    "\n",
    "1. Какой из anchor boxes, из 3 нарисованных вокруг клетки, нам подходит больше всего и как его можно немного подправить для того, чтобы он хорошо вписывал в себя объект\n",
    "2. Какой объект находится внутри этого anchor box и есть ли он вообще"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image](q_6ehzuef4yaip17avcipq8wevg.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Как определяется objectness?\n",
    "---\n",
    " Этот параметр определяется с помощью метрики IoU во время обучения. Для нее определяется порог по которому bounding box будут исключены"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Архетиктура\n",
    "---\n",
    "В начале идет feature extraction и в конце стоит CNN слой который определяет что в клетках на которые было разделено изображение"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Пример архетиктуры"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image](bjldwmnqycbox81gunaedgon4go.jpeg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Слой детекции\n",
    "---\n",
    "Как мы видим из картинки, этот слой, размером 13x13 (для картинок изначального размера 416x416) для того, чтобы рассказывать про «каждую клетку» на картинке. Из этого последнего слоя и достается информация, которую мы хотим."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# YOLO предсказывает 5 параметров (для каждого anchor box для определенной клетки):\n",
    "Задача YOLO — максимально точно предсказать эти параметры, чтобы максимально точно определять объект на картинке. А confidence score, который определяется для каждого предсказанного bounding box, является неким фильтром для того, чтобы отсеять совсем неточные предсказания. Для каждого предсказанного bounding box мы умножаем его IoU на вероятность того, что это определенный объект (вероятностное распределение рассчитывается во время обучения нейронной сети), берем лучшую вероятность из всех возможных, и если число после умножения превышает определенный порог, то мы можем оставить этот предсказанный bounding box на картинке."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image](Screenshot2023-05-1845450.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# После отсеивания ложных предсказаний\n",
    "---\n",
    "Дальше, когда у нас остались только предсказанные bounding boxes с высоким confidence score, наши предсказания (если их визуализировать) могут выглядеть примерно вот так:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image](we5nwbmeg9zm-jjuuh6hk79mkpa.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# NMS (non-max suppression)\n",
    "---\n",
    "Мы можем теперь использовать технику NMS (non-max suppression), чтобы отфильтровать bounding boxes таким образом, чтобы для одного объекта был только один предсказанный bounding box.\n",
    "Процесс NMS выполняется после того, как алгоритм обнаружения вернул прямоугольные области (bounding boxes) и соответствующие им оценки уверенности (confidence scores) для каждого обнаруженного объекта. Цель NMS заключается в отсеве повторяющихся или перекрывающихся прямоугольных областей, оставляя только наиболее вероятные и уникальные объекты.\n",
    "\n",
    "Основные шаги алгоритма NMS:\n",
    "\n",
    "1. Сортировка всех обнаруженных прямоугольных областей по их оценкам уверенности (confidence scores) в порядке убывания.\n",
    "2. Выбор области с наивысшей оценкой уверенности и добавление ее в окончательный список результатов. Эта область будет считаться \"наилучшей\" и будет служить представителем для других областей, которые с ней пересекаются.\n",
    "3. Вычисление значений IoU (Intersection over Union) между наилучшей областью и всеми остальными областями в списке.\n",
    "4. Удаление всех областей, у которых значение IoU выше определенного порога (обычно это значение между 0,5 и 0,7), поскольку они сильно перекрываются с наилучшей областью.\n",
    "5. Повторение шагов 2-4 до тех пор, пока не будут обработаны все прямоугольные области в отсортированном списке.\n",
    "\n",
    "\n",
    "Author: ChatGPT"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image](xpyyaqebpykn1y0h2_mgwbdaxum.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Разные скейлы изображений в новых архетиктурах\n",
    "---\n",
    "YOLOv3-4 предсказывают на 3-х разных скейлах. То есть картинка делится на 64 grid cells, на 256 клеток и на 1024 клетки, чтобы также видеть маленькие объекты. Для каждой группы клеток алгоритм повторяет необходимые действия во время предсказания/обучения, которые были описаны сверху."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Архетиктура YOLOv8\n",
    "---\n",
    "В данный момент соверменная архетиктура YOLOv8 использует свой собственный backbone и 3 скейла изображения"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![image](232463267-9c83fb29-83da-4782-90ae-d142f9aa77d8.jpg)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Использльваоние модели YOLOv8 для обучения и предсказания"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-05-17T13:18:09.941898600Z",
     "start_time": "2023-05-17T13:07:30.464130500Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.104  Python-3.9.13 torch-2.0.1+cu118 CUDA:0 (NVIDIA GeForce RTX 3070 Ti Laptop GPU, 8192MiB)\n",
      "\u001B[34m\u001B[1myolo\\engine\\trainer: \u001B[0mtask=detect, mode=train, model=yolov8n.pt, data=C:\\Users\\egors\\PycharmProjects\\object_detection\\Codev.v3i.yolov8\\data.yaml, epochs=20, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=0, workers=4, project=None, name=None, exist_ok=False, pretrained=False, optimizer=SGD, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=0, resume=False, amp=True, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, v5loader=False, tracker=botsort.yaml, save_dir=runs\\detect\\train8\n",
      "Overriding model.yaml nc=80 with nc=21\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    755407  ultralytics.nn.modules.head.Detect           [21, [64, 128, 256]]          \n",
      "Model summary: 225 layers, 3014943 parameters, 3014927 gradients\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "\u001B[34m\u001B[1mAMP: \u001B[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
      "\u001B[34m\u001B[1mAMP: \u001B[0mchecks passed \n",
      "\u001B[34m\u001B[1moptimizer:\u001B[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias\n",
      "\u001B[34m\u001B[1mtrain: \u001B[0mScanning C:\\Users\\egors\\PycharmProjects\\object_detection\\Codev.v3i.yolov8\\train\\labels.cache... 2415 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2415/2415 [00:00<?, ?it/s]\n",
      "\u001B[34m\u001B[1mval: \u001B[0mScanning C:\\Users\\egors\\PycharmProjects\\object_detection\\Codev.v3i.yolov8\\valid\\labels.cache... 798 images, 0 backgrounds, 0 corrupt: 100%|██████████| 798/798 [00:00<?, ?it/s]\n",
      "Plotting labels to runs\\detect\\train8\\labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001B[1mruns\\detect\\train8\u001B[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       1/20       2.2G     0.9922      3.977      1.387         65        640: 100%|██████████| 151/151 [00:20<00:00,  7.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:07<00:00,  3.33it/s]\n",
      "                   all        798       1801       0.32       0.12     0.0615     0.0438\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       2/20       2.1G      1.039      3.297       1.41        100        640: 100%|██████████| 151/151 [00:18<00:00,  8.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:07<00:00,  3.48it/s]\n",
      "                   all        798       1801       0.15       0.23      0.108     0.0656\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       3/20       2.1G      1.116      2.896       1.46         63        640: 100%|██████████| 151/151 [00:18<00:00,  8.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:07<00:00,  3.41it/s]\n",
      "                   all        798       1801      0.213       0.29      0.187      0.113\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       4/20       2.1G      1.149      2.749      1.483         59        640: 100%|██████████| 151/151 [00:18<00:00,  8.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:07<00:00,  3.48it/s]\n",
      "                   all        798       1801      0.242      0.286      0.214      0.126\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       5/20       2.1G      1.167      2.682      1.493         54        640: 100%|██████████| 151/151 [00:18<00:00,  8.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:07<00:00,  3.28it/s]\n",
      "                   all        798       1801      0.314      0.302      0.226      0.139\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       6/20       2.1G      1.132      2.532      1.466         68        640: 100%|██████████| 151/151 [00:18<00:00,  8.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:07<00:00,  3.44it/s]\n",
      "                   all        798       1801      0.385      0.302      0.253       0.16\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       7/20       2.1G      1.093      2.349      1.438         39        640: 100%|██████████| 151/151 [00:18<00:00,  8.21it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:07<00:00,  3.45it/s]\n",
      "                   all        798       1801      0.372      0.422      0.347      0.235\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       8/20       2.1G      1.087      2.278      1.433         67        640: 100%|██████████| 151/151 [00:18<00:00,  8.27it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:07<00:00,  3.48it/s]\n",
      "                   all        798       1801      0.378      0.354      0.325      0.215\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "       9/20       2.1G      1.048       2.14       1.41         67        640: 100%|██████████| 151/151 [00:18<00:00,  8.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:07<00:00,  3.45it/s]\n",
      "                   all        798       1801      0.393      0.386      0.354      0.239\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      10/20       2.1G       1.03      2.073      1.401         71        640: 100%|██████████| 151/151 [00:18<00:00,  8.18it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:07<00:00,  3.48it/s]\n",
      "                   all        798       1801      0.513      0.476      0.456      0.315\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      11/20       2.1G      1.009      2.008      1.381         58        640: 100%|██████████| 151/151 [00:18<00:00,  8.17it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:07<00:00,  3.49it/s]\n",
      "                   all        798       1801      0.522      0.484      0.477       0.34\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      12/20      2.17G     0.9945      1.932      1.366         56        640: 100%|██████████| 151/151 [00:18<00:00,  8.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:07<00:00,  3.48it/s]\n",
      "                   all        798       1801      0.526       0.47      0.474      0.335\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      13/20      2.09G     0.9569       1.82      1.342         48        640: 100%|██████████| 151/151 [00:18<00:00,  8.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:07<00:00,  3.49it/s]\n",
      "                   all        798       1801      0.579      0.504      0.539      0.386\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      14/20       2.1G     0.9525      1.782      1.334         55        640: 100%|██████████| 151/151 [00:18<00:00,  8.25it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:07<00:00,  3.48it/s]\n",
      "                   all        798       1801      0.613      0.507      0.549      0.397\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      15/20       2.1G     0.9257      1.674      1.313         71        640: 100%|██████████| 151/151 [00:18<00:00,  8.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:07<00:00,  3.44it/s]\n",
      "                   all        798       1801      0.629      0.556       0.59      0.438\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      16/20       2.1G     0.9072      1.625      1.306         70        640: 100%|██████████| 151/151 [00:18<00:00,  8.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:07<00:00,  3.48it/s]\n",
      "                   all        798       1801      0.632      0.554      0.597      0.439\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      17/20       2.1G     0.9111      1.583      1.298         41        640: 100%|██████████| 151/151 [00:18<00:00,  8.34it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:07<00:00,  3.48it/s]\n",
      "                   all        798       1801      0.669      0.523      0.596      0.445\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      18/20       2.1G     0.8925      1.528      1.281         80        640: 100%|██████████| 151/151 [00:18<00:00,  8.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 25/25 [00:07<00:00,  3.45it/s]\n",
      "                   all        798       1801      0.677      0.574      0.628      0.474\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "      19/20       2.1G      0.867      1.497      1.276         45        640:  35%|███▌      | 53/151 [00:06<00:11,  8.29it/s]"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Use the model\n",
    "model.train(data=\"Codev.v3i.yolov8\\\\data.yaml\", epochs=20,workers=4,device=0)  # train the model\n",
    "metrics = model.val()  # evaluate model performance on the validation set\n",
    "results = model.predict(source='Codev.v3i.yolov8\\\\test\\\\images',save=True)\n",
    "success = model.export(format=\"onnx\")  # export the model to ONNX format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
